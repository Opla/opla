{
  "settings": {
    "start_app": true,
    "welcome_splash": true,
    "selected_page": "/threads"
  },
  "models": {
    "path": "dev/ai/models",
    "active_model": null,
    "items": []
  },
  "server": {
    "name": "llama.cpp",
    "launch_at_startup": true,
    "binary": "binaries/llama.cpp/llama.cpp.server",
    "parameters": {
      "host": "127.0.0.1",
      "port": 8081,
      "context_size": 512,
      "threads": 6,
      "n_gpu_layers": 0
    }
  },
  "downloads": []
}
